{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import nltk \n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk import word_tokenize, pos_tag\n",
    "# from nltk.stem import PorterStemmer\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "pd.options.display.max_rows = 500\n",
    "# Read file\n",
    "df1 = pd.read_pickle('data_prep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us see the top 20 again after cleaning\n",
    "freq_NN = pd.Series(' '.join(df1['cleaned']).split()).value_counts()[:20]\n",
    "freq_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "font = './simsun.ttf'\n",
    "wordcloud = WordCloud(background_color='white',width=800, height=500, random_state=21, max_font_size=110, font_path=font).generate(str(df1['cleaned'])) \n",
    "plt.figure(1,figsize=(13, 10))\n",
    "fig=plt.figure(1)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "fig.savefig('wordcloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use CountVectorizer to get bigrams to visualize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stop_words = open('./ch-stop-words.txt').read().split('\\n')\n",
    "# 作业二： 请补全下方函数中的参数，参数设置如下：\n",
    "# 最大文档频：0.95\n",
    "# 最小文档数：2\n",
    "# 停用词表： stop_words\n",
    "# 最大特征数：10000\n",
    "# 考虑的 n-gram 范围：1-3\n",
    "\n",
    "# (sklearn官方文档：https://scikit-learn.org/stable/modules/classes.html)\n",
    "cv = CountVectorizer( ... )\n",
    "X = cv.fit_transform(df1['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Most frequently occuring words\n",
    "def get_top_n_words(corpus,n=None):\n",
    "    vec = CountVectorizer().fit(df1['cleaned'])\n",
    "    bag_of_words=vec.transform(df1['cleaned'])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word , idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "# Convert most freq words to datafame for visuals\n",
    "\n",
    "top_words = get_top_n_words(df1['cleaned'], n=20)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns = ['Word', 'Freq']\n",
    "\n",
    "# barplot of most freq words\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "myfont=FontProperties(fname='./simsun.ttf',size=14)\n",
    "sns.set(rc={'figure.figsize':(13,6)}, font=myfont.get_name())\n",
    "g = sns.barplot(x='Word', y='Freq', data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "fig = g.get_figure()\n",
    "fig.savefig(\"sns_output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently Co-occuring words\n",
    "def get_top_n2_words(corpus,n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),max_features=2000).fit(df1['data_nouns'])\n",
    "    bag_of_words=vec1.transform(df1['data_nouns'])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word , idx in vec1.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "# Convert most freq words to datafame for visuals\n",
    "\n",
    "top2_words = get_top_n2_words(df1['data_nouns'], n=20)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns = ['Bi-gram', 'Freq']\n",
    "\n",
    "# for bigram wordcloud\n",
    "top100_words = get_top_n2_words(df1['data_nouns'], n=100)\n",
    "top100_df = pd.DataFrame(top100_words)\n",
    "top100_df.columns = ['Bi-gram', 'Freq']\n",
    "print(top2_df)\n",
    "\n",
    "# barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,6)},font=myfont.get_name())\n",
    "h = sns.barplot(x='Bi-gram', y='Freq', data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)\n",
    "fig = h.get_figure()\n",
    "fig.savefig(\"sns_output_bigrams.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_df['Bi_gram'] = top100_df['Bi-gram'].str.replace(' ','_')\n",
    "top100_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, background_color='white', font_path=font).generate(str(top100_df['Bi_gram'])) \n",
    "plt.figure(1,figsize=(13, 10))\n",
    "fig=plt.figure(1)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\") \n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "fig.savefig('wordcloud_bigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words= stop_words, max_features=no_features, ngram_range=(1,1),\n",
    "                                  analyzer='word')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df1['cleaned'])\n",
    "# get the feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary has %d distinct terms' % len(tfidf_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms with the highest TF-IDF scores across all documents in the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def rank_terms( tfidf, tfidf_feature_names ):\n",
    "    # get the sums over each column\n",
    "    sums = tfidf.sum(axis=0)\n",
    "    # map weights to the terms\n",
    "    weights = {}\n",
    "    for col, term in enumerate(tfidf_feature_names):\n",
    "        weights[term] = sums[0,col]\n",
    "    # rank the terms by their weight over all documents\n",
    "    return sorted(weights.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = rank_terms(tfidf, tfidf_feature_names)\n",
    "for i, pair in enumerate( ranking[0:20] ):\n",
    "    print( \"%02d. %s (%.0f)\" % ( i+1, pair[0], pair[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF model fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作业三：请补全下方 NMF 函数中的参数，参数设置如下：\n",
    "# 主题数： no_topics\n",
    "# 随机数种子：1\n",
    "# 正则项权重：0.1\n",
    "# L1正则项权重：0.5\n",
    "# 初始化方法：nndsvd\n",
    "\n",
    "# (sklearn官方文档：https://scikit-learn.org/stable/modules/classes.html)\n",
    "nmf=NMF( ... ).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    col1 = 'topic'\n",
    "    col2 = 'top_ten_words'\n",
    "    dct = {col1: [], col2: []}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        dct[col1].append(str(topic_idx))\n",
    "        dct[col2].append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return pd.DataFrame.from_dict(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "topic_word = display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words = 3\n",
    "topic_word_3 = display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "topic_word_3['Theme'] = topic_word_3.top_ten_words.str.title()\n",
    "topic_word_4 = topic_word_3.loc[:,['topic', 'Theme']]\n",
    "topic_word_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the theme with the top_ten\n",
    "topic_word = pd.merge(topic_word_4, topic_word, on ='topic')\n",
    "topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_W = nmf.transform(tfidf)\n",
    "nmf_H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'topic': nmf_W.argmax(axis=1),\n",
    "                   'Doc_Id': df1['Doc_Id'],\n",
    "                   'content': df1['content']},\n",
    "                  columns = ['topic', 'Doc_Id', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showdocs(df2, topics, nshow=5 ): # nshow= len(df1)\n",
    "    idx = df2.topic == topics[0]\n",
    "    for i in range(1, len(topics)):\n",
    "        idx = idx | (df2.topic == topics[i])\n",
    "    return df2[idx].groupby('topic').head(nshow).sort_values('topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = tuple(list(range(0, no_topics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = showdocs(df2, numbers)\n",
    "final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for use later\n",
    "final.to_pickle('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.to_pickle('topic_word.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
